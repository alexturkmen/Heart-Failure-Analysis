<!DOCTYPE html>
<html lang="en">
  <title>The Quad++ Final Project</title>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" />
  <link
    rel="stylesheet"
    href="https://fonts.googleapis.com/css?family=Montserrat"
  />
  <link
    rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"
  />

  <!-- Style -->
  <style>
    body,
    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
      font-family: "Lato", sans-serif;
    }
    .w3-bar,
    h1,
    button {
      font-family: "Montserrat", sans-serif;
    }
    .fa-heart,
    .fa-coffee {
      font-size: 200px;
    }
    tbody tr:nth-child(even), tbody thead tr th td {
      background-color: rgba(169, 177, 172, 0.685);
    }
  </style>
  <body>
    <!-- Navbar Reg Screen -->
    <div class="w3-top">
      <div class="w3-bar w3-red w3-card w3-left-align w3-large">
        <a
          class="w3-bar-item w3-button w3-hide-medium w3-hide-large w3-right w3-padding-large w3-hover-white w3-large w3-red"
          href="javascript:void(0);"
          onclick="myFunction()"
          title="Toggle Navigation Menu"
          ><i class="fa fa-bars"></i
        ></a>
        <a href="/" class="w3-bar-item w3-button w3-padding-large w3-white"
          >Home</a
        >
        <a
          href="/link1"
          class="w3-bar-item w3-button w3-hide-small w3-padding-large w3-hover-white"
          >Model</a
        >
        <a
          href="/link2"
          class="w3-bar-item w3-button w3-hide-small w3-padding-large w3-hover-white"
          >Results</a
        >
        <a
          href="/viz"
          class="w3-bar-item w3-button w3-hide-small w3-padding-large w3-hover-white"
          >Visualizations</a
        >
        <a
          href="/team"
          class="w3-bar-item w3-button w3-hide-small w3-padding-large w3-hover-white"
          >Team</a
        >
      </div>

      <!-- Navbar on small screens -->
      <div
        id="navDemo"
        class="w3-bar-block w3-white w3-hide w3-hide-large w3-hide-medium w3-large"
      >
        <a href="/link1" class="w3-bar-item w3-button w3-padding-large"
          >Model</a
        >
        <a href="/link2" class="w3-bar-item w3-button w3-padding-large"
          >Results</a
        >
        <a href="/viz" class="w3-bar-item w3-button w3-padding-large"
          >Visualizations</a
        >
        <a href="/team" class="w3-bar-item w3-button w3-padding-large">Team</a>
      </div>
    </div>

    <!-- Header -->
    <header class="w3-container w3-red w3-center" style="padding: 64px 16px">
      <h1 class="w3-margin w3-jumbo">MODELING LOGISTIC REGRESSION</h1>
      <p class="w3-xlarge">A ML Project by The Quad++</p>
    </header>

<!-- Logistic Regression | Section 1 -->
    <div class="w3-row-padding w3-padding-64 w3-container">
      <div class="w3-content">
        <div class="w3-col">
          <h1>What is Logistic Regression</h1>
          <h6 class="w3-padding-16">
            Logistic Regression is a Machine Learning algorithm which is used
            for the classification problems, it is a predictive analysis
            algorithm and based on the concept of probability. Another way to
            describe it is, a statistical method for predicting binary outcomes
            from data. Please see a simple chart below to show difference
            between Linear Regression and Logistic Regression.<br><br>

            We choose Logistic Regression as one of the models to fit our data,
            because the Heart Failure Data we analyze is comprised of a mix of
            categorical and numerical variables for input. It also yields one
            categorical output, which has "0" or "1" for an answer. Therefore,
            Logistic Regression seems to be a suitable model. 
          </h6>
          <p style="text-align: center">
            <img
              src="/static/images/log_reg/log_reg1.jpeg"
              alt="Ida"
              style="width: 80%; padding-bottom: 40px"
            />
          </p></div></div></div>

    <!-- Random Forest Classification | Section 3 -->
    <div class="w3-row-padding w3-light-grey w3-padding-64 w3-container">
      <div class="w3-content">
        <div class="w3-col">
          <h1>What is Random Forest Classification</h1>
          <h6 class="w3-padding-16">
            Random forests or random decision forests are an ensemble learning method for classification, 
            regression and other tasks that operate by constructing a multitude of decision trees at training time 
            and outputting the class that is the mode of the classes (classification) or mean/average 
            prediction (regression) of the individual trees. Random decision forests correct for decision trees' 
            habit of overfitting to their training set.<br/><br/>

            Random forest is a classic classification algorithm, which is why it was chosen for this analysis.
          </h6>
          <h6 class="w3-padding-16">
            <a href="https://williamkoehrsen.medium.com/random-forest-simple-explanation-377895a60d2d" target="_blank">Source 1</a> | 
            <a href="https://en.wikipedia.org/wiki/Random_forest" target="_blank">Source 2</a>
          </h6>
          <p style="text-align: center">
            <img
              src="/static/images/random-forest-classifier.png"
              alt="Ida"
              style="width: 60%; padding-bottom: 40px"
            />
          </p></div></div></div>

    <!-- K-Nearest Neighbors | Section 4 -->
    <div class="w3-row-padding w3-padding-64 w3-container">
      <div class="w3-content">
        <div class="w3-col">
          <h1>What is K-Nearest Neighbors</h1>
          <h6 class="w3-padding-16">
            The K-Nearest Neighbors algorithm is a non-parametric classification method first developed by Evelyn Fix 
            and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. 
            In both cases, the input consists of the k closest training examples in data set.<br/><br/>

            KNN is a lazy learning method, which means there is no explicit training phase before classification. Since the generalization
            and/or abstraction of the data happens at classification the entire dataset must be kept in memory. Along with the computationally 
            intense nature of the KNN algorithm, this makes the application to small datasets with a relatively low number of features very attractive.
            
          </h6>
          <h6 class="w3-padding-16">
            <a href="https://medium.com/machine-learning-101/k-nearest-neighbors-classifier-1c1ff404d265" target="_blank">Source 1</a> | <a href="https://towardsdatascience.com/introduction-to-k-nearest-neighbors-3b534bb11d26" target="_blank">Source 2</a> | <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" target="_blank">Source 3</a>
            
          </h6>
          <p style="text-align: center">
            <img
              src="/static/images/knn-classifier-diagram.png"
              alt="Ida"
              style="width: 80%; padding-bottom: 40px"
            />
          </p></div></div></div>

   <!-- Multilayer Perceptron Classification | Section 5 -->
   <div class="w3-row-padding w3-light-grey w3-padding-64 w3-container">
    <div class="w3-content">
      <div class="w3-col">
        <h1>What is Multilayer Perceptron Classification</h1>
        <h6 class="w3-padding-16">
          A multi layer perceptron (MLP) is a class of feed forward artificial neural network. 
          MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. 
          Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP 
          utilizes a supervised learning technique called back propagation for training. (Wikipedia) <br/><br/>

          The MLP classifier performs binary classification, which is our use case for this analysis. They are particulary good at solving
          problems stochastically. Since health information sampled from the population can be assumed to have a relatively random distribution
          this classifier was chosen. MLPs are especially susceptible to the scaling of your dataset, so we chose to normalize the features from
          0 to 1 using scikit-learn MinMaxScaler. 
        </h6>
        <h6 class="w3-padding-16">
          <a href="https://becominghuman.ai/multi-layer-perceptron-mlp-models-on-real-world-banking-data-f6dd3d7e998f" target="_blank">Source 1</a>
        </h6>
        <p style="text-align: center">
          <img
            src="/static/images/mlp_classifier.diagram.png"
            alt="Ida"
            style="width: 60%; padding-bottom: 40px"
          />
        </p></div></div></div>


    <!-- Gaussian Naive-Bayes | Section 6 -->
    <div class="w3-row-padding w3-padding-64 w3-container">
      <div class="w3-content">
        <div class="w3-col">
          <h1>What is Gaussian Naive Bayes</h1>
          <h6 class="w3-padding-16">
            Naive Bayes classifiers are a family of simple "probabilistic classifiers" based on applying Bayes' 
            theorem with strong (naïve) independence assumptions between the features. They are among the 
            simplest Bayesian network models, but coupled with kernel density estimation, they can achieve 
            higher accuracy levels.<br/><br/>

            Naïve Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of 
            variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating 
            a closed-form expression, which takes linear time, rather than by expensive iterative approximation as 
            used for many other types of classifiers.<br/><br/>

            When dealing with continuous data, a typical assumption is that the continuous values associated with each 
            class are distributed according to a normal (or Gaussian) distribution. Since the data in this analysis has both
            continous and discrete features, the Gaussian Naive-Bayes classifier on scikit-learn was selected. The majority of 
            features in this dataset are assumed to be distributed according to the normal distribution since there is a "healthy" 
            range for most of the features which would be the mean, and the discrete features are automatically handled by the scikit-learn 
            implementation.
          </h6>
          <h6 class="w3-padding-16">
            <a href="https://iq.opengenus.org/gaussian-naive-bayes/" target="_blank">Source 1</a> | <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" target="_blank">Source 2</a>

          </h6>
          <p style="text-align: center">
            <img
              src="/static/images/gnb_classifier-diagram.png"
              alt="Ida"
              style="width: 40%; padding-bottom: 40px"
            />
          </p></div></div></div>

          <!-- Sources -->
          <div class="w3-row-padding w3-padding-64 w3-container">
            <div class="w3-content">


          <h6 class="w3-padding-32" style="font-style: italic">
            Sources: <br /><a href="https://towardsdatascience.com/introduction-to-logistic-regression-66248243c148" target="_blank">
            1. Ayush Pant - Introduction to Logistic Regression </a><br />
          </h6>
        </div>
      </div>
    </div></div></div>

    <!-- Footer -->
    <footer class="w3-container w3-padding-32 w3-center w3-opacity">
      <a href="#" class="w3-button w3-black w3-margin"
        ><i class="fa fa-arrow-up w3-margin-right"></i>To the top</a
      >
    </footer>

    <script>
      // Used to toggle the menu on small screens when clicking on the menu button
      function myFunction() {
        var x = document.getElementById("navDemo");
        if (x.className.indexOf("w3-show") == -1) {
          x.className += " w3-show";
        } else {
          x.className = x.className.replace(" w3-show", "");
        }
      }
    </script>

    <script src="https://d3js.org/d3.v6.min.js"></script>
    <script src="{{ url_for('static', filename='js/app.js') }}"></script>
  </body>
</html>
